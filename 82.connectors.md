# Connectors[](#connectors)
* [从文件系统读取数据](#reading-from-file-systems)
    * [用Hadoop文件系统实现](#using-hadoop-file-system-implementations)
* [ 用HadoopInput/OutputFormat包装器连接到其他系统](#connecting-to-other-systems-using-inputoutputformat-wrappers-for-hadoop)
* [Flink对Avro的支持](#avro-support-in-flink)
    * [接入Microsoft Azure表存储](#access-microsoft-azure-table-storage)
* [接入MongoDB](#access-mongodb)

## Reading from file systems[](#reading-from-file-systems)
***
Flink 已经内置的支持如下文件系统:

|Filesystem|Scheme|Notes|
|:-|:-|:-|
|Hadoop Distributed File System (HDFS) |hdfs://|All HDFS versions are supported|
|Amazon S3|s3://|Support through Hadoop file system implementation (see below)|
|MapR file system|maprfs://|The user has to manually place the required jar files in the lib/ dir|
|Alluxio|alluxio://|Support through Hadoop file system implementation (see below)|

### Using Hadoop file system implementations[](#using-hadoop-file-system-implementations)

Apache Flink 允许用户使用任何文件系统去实现`org.apache.hadoop.fs.FileSystem`接口。 Hadoop `FileSystem` 实现如下:

* [S3](https://aws.amazon.com/s3/) (tested)
* [Google Cloud Storage Connector for Hadoop](https://cloud.google.com/hadoop/google-cloud-storage-connector) (tested)
* [Alluxio](http://alluxio.org/) (tested)
* [XtreemFS](http://www.xtreemfs.org/) (tested)
*   FTP via [Hftp](http://hadoop.apache.org/docs/r1.2.1/hftp.html) (not tested)
*   and many more.

为了在Hadoop文件系统上使用Flink, 请务必确保：

* 文件`flink-conf.yaml` 已经设置了 `fs.hdfs.hadoopconf` 属性到Hadoop配置目录。为了在IDE中进行自动化测试和运行,需要定义 `FLINK_CONF_DIR`环境变量来设置包含 `flink-conf.yaml` 的目录。
* 对于所需文件系统,Hadoop有一个配置实体(那个目录中)在 `core-site.xml`中. 例如 S3 and Alluxio 是被链接/展示如下.
* 为了使用文件系统,一些必须的类可以在Flink安装目录的(所有正在运行Flink的机器)`lib/` 文件夹中找到 . 如果无法把文件放到目录中, Flink也参照 `HADOOP_CLASSPATH` 环境变把到Hadoop jar 文件中添加到类路径中。

#### Amazon S3[](#amazon-s3)

参照[Deployment &amp; Operations - Deployment - AWS - S3: Simple Storage Service](//ci.apache.org/projects/flink/flink-docs-release-1.6/ops/deployment/aws.html)中使用S3文件系统中的实现方法来配置条目和库依赖。

#### Alluxio[](#alluxio)
为了支持Alluxio需要添加如下依赖到`core-site.xml` 文件:
```
<property>
  <name>fs.alluxio.impl</name>
  <value>alluxio.hadoop.FileSystem</value>
</property>
```

## 使用Hadoop的输入/输出格式包装器连接到其他系统[](#connecting-to-other-systems-using-inputoutputformat-wrappers-for-hadoop)
***
Apache Flink允许用户接入许多不同的系统作为数据源或sinks。良好的系统设计使得非常容易进行扩展。与Apache Hadoop类似,Flink也有 `InputFormat` and `OutputFormat`的概念。
`InputFormat`s的其中一个实现就是`HadoopInputFormat`s。这个包装器允许Flink接入所有已有的Hadoop输入格式。
这块展示了一些Flink连接到其他系统的例子。[Read more about Hadoop compatibility in Flink](//ci.apache.org/projects/flink/flink-docs-release-1.6/dev/batch/hadoop_compatibility.html).

## Avro support in Flink[](#avro-support-in-flink)
***
Flink对[Apache Avro](http://avro.apache.org/)有广泛的内置支持.可以使Flink很容易地从Avro文件读取数据。Flink序列化框架也能处理Avro schemas产生的类。前提是确保已将Flink Avro依赖添加到项目的pom.xml文件中。

```
<dependency>
  <groupId>org.apache.flink</groupId>
  <artifactId>flink-avro</artifactId>
  <version>1.6.0</version>
</dependency>
```

为了从Avro文件读取数据, 你必须指定一个`AvroInputFormat`。

**Example**:

```
AvroInputFormat<User> users = new AvroInputFormat<User>(in, User.class);
DataSet<User> usersDS = env.createInput(users);
```
注意，`User`是Avro产生的一个POJO。Flink也支持对这些POJO进行string-based key选择操作。例如：
```
usersDS.groupBy("name")
```
需要指出的是，Flink使用`GenericData.Record`类型也是可以的，但是并不推荐这么做。因为这条记录包含了全部的schema,属于数据密集型计算因此会很慢。
Flink的POJO字段选择也支持Avro产生的POJO。然而,这种用法仅在产生的类的字段类型被正确填下的时候可以。如果这字段是类型`Object`的，就不能把这个字段用作join or grouping key。像这样 `{"name": "type_double_test", "type": "double"},`在Avro指定字段可以很好的工作, 然而， 仅用一个字段 (`{"name": "type_double_test", "type":["double"]},`)指定它作为一个UNION-type 就会产生 `Object`类型. 注意，指定nullable类型 (`{"name": "type_double_test", "type": ["null", "double"]},`) 也是可以的。

### 接入 Microsoft Azure 表存储[](#access-microsoft-azure-table-storage)

_Note: 从Flink 0.6-incubating才开始支持_

这个例子展示使用 `HadoopInputFormat` 包装器实现接入[Azure’s Table Storage](https://azure.microsoft.com/en-us/documentation/articles/storage-introduction/).

1.  下载和编译 `azure-tables-hadoop` 项目. 在Maven仓库中还没有被开发的输入格式, 因此，我们必须靠自己建立项目,执行如下命令：
```
git clone https://github.com/mooso/azure-tables-hadoop.git
cd azure-tables-hadoop
mvn clean install
```

1.  使用quickstarts启动一个新的Flink项目:
```
curl https://flink.apache.org/q/quickstart.sh | bash
```
1.  添加如下依赖 (在 `<dependencies>` 中) 到你的 `pom.xml` 文件:

```
 <dependency>
       <groupId>org.apache.flink</groupId>
       <artifactId>flink-hadoop-compatibility_2.11</artifactId>
       <version>1.6.0</version>
   </dependency>
   <dependency>
     <groupId>com.microsoft.hadoop</groupId>
     <artifactId>microsoft-hadoop-azure</artifactId>
     <version>0.0.4</version>
   </dependency>
```
`flink-hadoop-compatibility` 是一个Flink package，它提供了Hadoop 输入格式包装器。新建项目的时候`microsoft-hadoop-azure`被添加到项目中。

这项目现在已经准备好开始编码了。我们建议将项目导入IDE，例如Eclipse或ItLyLJ。（作为Maven项目导入！）浏览到Jo.java文件的代码。现在还是一个空的Flink作业。
将下面的代码粘贴进来：

```
import java.util.Map;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.java.DataSet;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import com.microsoft.hadoop.azure.AzureTableConfiguration;
import com.microsoft.hadoop.azure.AzureTableInputFormat;
import com.microsoft.hadoop.azure.WritableEntity;
import com.microsoft.windowsazure.storage.table.EntityProperty;

public class AzureTableExample {

  public static void main(String[] args) throws Exception {
    // set up the execution environment
    final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();

    // create a  AzureTableInputFormat, using a Hadoop input format wrapper
    HadoopInputFormat<Text, WritableEntity> hdIf = new HadoopInputFormat<Text, WritableEntity>(new AzureTableInputFormat(), Text.class, WritableEntity.class, new Job());

    // set the Account URI, something like: https://apacheflink.table.core.windows.net
    hdIf.getConfiguration().set(AzureTableConfiguration.Keys.ACCOUNT_URI.getKey(), "TODO");
    // set the secret storage key here
    hdIf.getConfiguration().set(AzureTableConfiguration.Keys.STORAGE_KEY.getKey(), "TODO");
    // set the table name here
    hdIf.getConfiguration().set(AzureTableConfiguration.Keys.TABLE_NAME.getKey(), "TODO");

    DataSet<Tuple2<Text, WritableEntity>> input = env.createInput(hdIf);
    // a little example how to use the data in a mapper.
    DataSet<String> fin = input.map(new MapFunction<Tuple2<Text,WritableEntity>, String>() {
      @Override
      public String map(Tuple2<Text, WritableEntity> arg0) throws Exception {
        System.err.println("--------------------------------\nKey = "+arg0.f0);
        WritableEntity we = arg0.f1;

        for(Map.Entry<String, EntityProperty> prop : we.getProperties().entrySet()) {
          System.err.println("key="+prop.getKey() + " ; value (asString)="+prop.getValue().getValueAsString());
        }

        return arg0.f0.toString();
      }
    });

    // emit result (this works only locally)
    fin.print();

    // execute program
    env.execute("Azure Example");
  }
}
```
该示例演示如何接入Azure表并将数据转换为Flink的`DataSet` （更具体地说，该数据集类型是`<Tuple2<Text,WritableEntity>`）。使用`DataSet`，可以将所有已知的转换应用到数据集。

## Access MongoDB[](#access-mongodb)

This [GitHub repository documents how to use MongoDB with Apache Flink (starting from 0.7-incubating)](https://github.com/okkam-it/flink-mongodb-test)
[<span class="glyphicon glyphicon-chevron-up"></span> Back to top](#top)